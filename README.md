# IA Coding - Code Generator

A Rust CLI application that generates source code from natural language descriptions using the `llama_cpp` crate.

## Description

This project provides a command-line interface where users can enter natural language descriptions (e.g., "Create a for loop in Rust iterating from 0 to 10"), and the application will generate corresponding Rust code using a language model.

## Features

- ðŸš€ Simple and intuitive CLI interface
- ðŸ›¡ï¸ Error handling for invalid inputs and model failures
- ðŸ”„ Interactive session support (keep entering descriptions until you exit)
- âœ… Input validation and error messages
- ðŸ§ª Unit tests for core functionality

## Prerequisites

- Rust 1.91.1 or later
- Cargo (comes with Rust)
- A GGUF format language model file (for full functionality)

## Installation

1. Clone the repository:
```bash
git clone https://github.com/vince59/ia_coding.git
cd ia_coding
```

2. Build the project:
```bash
cargo build --release
```

## Usage

### Running the CLI

To start the interactive CLI:

```bash
cargo run
```

Or, if you built the release version:

```bash
./target/release/ia_coding
```

### Using the CLI

1. The application will display a welcome message
2. Enter your natural language description when prompted
3. The generated code will be displayed
4. Type `quit` or `exit` to leave the application

### Example Session

```
=== IA Coding - Code Generator ===
Generate Rust code from natural language descriptions
Type 'quit' or 'exit' to leave

Enter your description (e.g., 'Create a for loop in Rust iterating from 0 to 10'): Create a for loop in Rust iterating from 0 to 10

--- Generated Code ---
[Generated code will appear here]
--- End of Generated Code ---

Enter your description: exit
Goodbye!
```

## Model Configuration

### Setting up llama_cpp with a Model

The current implementation uses the `llama_cpp` crate (version 0.3.2). To enable full code generation functionality, you need to:

1. **Download a GGUF Model:**
   - Visit [Hugging Face](https://huggingface.co/models) or other model repositories
   - Download a compatible GGUF format model (e.g., CodeLlama, Llama 2, or similar models trained for code generation)
   - Recommended: Use a code-specific model like `codellama-7b-instruct.Q4_K_M.gguf`

2. **Place the Model:**
   - Create a `models` directory in the project root
   - Place your downloaded model file there

3. **Update the Code:**
   - Modify the `generate_code` function in `src/main.rs` to load and use your model
   - Example integration code is provided in the comments

### Example Model Integration

```rust
use llama_cpp::{LlamaModel, LlamaParams, CompletionParams};

fn generate_code(description: &str) -> Result<String, String> {
    // Load the model
    let model = LlamaModel::load_from_file(
        "models/your-model.gguf",
        LlamaParams::default()
    ).map_err(|e| format!("Failed to load model: {}", e))?;

    // Create a prompt
    let prompt = format!("Generate Rust code for: {}", description);

    // Generate completion
    let completion = model.complete(
        &prompt,
        CompletionParams::default()
    ).map_err(|e| format!("Failed to generate: {}", e))?;

    Ok(completion.text)
}
```

## Testing

Run the test suite:

```bash
cargo test
```

Run tests with output:

```bash
cargo test -- --nocapture
```

## Error Handling

The application includes error handling for:

- Empty input descriptions
- Input that is too short (less than 5 characters)
- Model loading failures
- Model inference failures
- Invalid user input

## Project Structure

```
ia_coding/
â”œâ”€â”€ Cargo.toml          # Project dependencies and metadata
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.rs         # Main CLI application and code generation logic
â””â”€â”€ target/             # Build artifacts (generated by cargo)
```

## Dependencies

- `llama_cpp` (0.3.2) - High-level bindings to llama.cpp for running language models

## Development

### Building

```bash
# Debug build
cargo build

# Release build (optimized)
cargo build --release
```

### Running

```bash
# Run in debug mode
cargo run

# Run in release mode
cargo run --release
```

### Linting

```bash
cargo clippy
```

### Formatting

```bash
cargo fmt
```

## Troubleshooting

### "Model integration pending" Error

This error appears when no model file is configured. Follow the "Model Configuration" section above to set up a model.

### Build Errors

If you encounter build errors:
1. Ensure you have the latest Rust toolchain: `rustup update`
2. Clean the build: `cargo clean`
3. Rebuild: `cargo build`

### Performance Issues

For better performance:
1. Use the release build: `cargo build --release`
2. Use a quantized model (e.g., Q4_K_M format) for better memory usage
3. Consider using GPU acceleration if available (configure llama_cpp features)

## Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## License

This project is open source. Please check the repository for license information.

## Acknowledgments

- Built with [llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)
- Powered by [llama.cpp](https://github.com/ggerganov/llama.cpp)
